
# ğŸ§  Mini Word Embedding from Scratch (Word2Vec-Style)

This project implements a **minimal Word2Vec-style word embedding model** from scratch using Python and NumPy. It uses the **Skip-gram architecture with Negative Sampling**, and visualizes word vectors using **PCA**.

---

## ğŸ“˜ Features

- ğŸ”¤ Text preprocessing and tokenization
- ğŸ“š Vocabulary creation
- ğŸ” Skip-gram training pair generation
- ğŸ§  Training with negative sampling (custom SGD)
- ğŸ“Š Word vector visualization in 2D space (PCA)

---

## ğŸš€ Quick Start

### 1. Clone the repository

```bash
git clone https://github.com/your-username/mini-word-embedding.git
cd mini-word-embedding
```

### 2. Install dependencies

This project only requires basic libraries:

```bash
pip install numpy matplotlib scikit-learn tqdm
```

### 3. Run the notebook

```bash
jupyter notebook mini_word_embedding.ipynb
```

---

## ğŸ§ª Sample Output

Using this text:

```text
"king queen man woman king queen"
```

You'll train 10-dimensional word vectors and visualize them in 2D like this:

```
king     â—
queen    â—
man      â—
woman    â—
```

Words like `'king'` and `'queen'` will be placed close together if trained correctly.

---

## ğŸ“‚ File Structure

```
ğŸ“ mini-word-embedding/
â”œâ”€â”€ mini_word_embedding.ipynb     # Main notebook
â”œâ”€â”€ README.md                     # You're here
```

---

## ğŸ™Œ Credits

Inspired by the original Word2Vec paper (Mikolov et al., 2013).

Feel free to fork, modify, or share!
